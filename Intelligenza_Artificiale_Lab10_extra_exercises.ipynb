{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jJodIib1_E6tPBvi3BqoxczebfRDQSIC","timestamp":1683710199995}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## EXERCISE 1: What to do at the airport?\n","\n","You are travelling and have some time to kill at the aiport. There are three things you could spend your time doing:\n","  \n","1) You could have a coffee.\n","\n","This has a probability of $0.8$ of giving you time to relax with a tasty beverage, and a utility of $10$. \n","It also has a probability of $0.2$ of providing you with a nasty cup from over-roasted beans that annoys you,\n","and outcome with a utility of $-5$.\n","\n","2) You could shop for clothes.\n","\n","This has a probability of $0.1$ that you will find a great outfit at a good price, utility $20$. However, it \n","has a probability of $0.9$ that you end up wasting money on over-priced junk, utility $-10$.\n","\n","3) You could have a bite to eat.\n","\n","This has a probability of $0.8$ that you find something rather mediocre that prevents you from being too hungry \n","during your flight, utility $2$, and a probability of $0.2$ that you find something filling and tasty, utility $5$.\n","\n","> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?\n","\n","> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n","\n","> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n","    "],"metadata":{"id":"twbLWgpO7YQH"}},{"cell_type":"code","source":["import numpy as np\n","\n","coffee_outcomes=['relax','annoy']\n","u_coffee_outcomes=np.array([10,-5])\n","p_coffee_outcomes=np.array([0.8,0.2])\n","\n","eu_coffee=np.sum(p_coffee_outcomes*u_coffee_outcomes)\n","\n","print(\"eu coffee: \", eu_coffee)\n","\n","clothes_outcomes=['good','wasting']\n","u_clothes_outcomes=np.array([20,-10])\n","p_clothes_outcomes=np.array([0.1,0.9])\n","\n","eu_clothes=np.sum(p_clothes_outcomes*u_clothes_outcomes)\n","\n","print(\"eu clothes: \",eu_clothes)\n","\n","eat_outcomes=['tasty','mediocre']\n","u_eat_outcomes=np.array([5,2])\n","p_eat_outcomes=np.array([0.8,0.2])\n","\n","eu_eat=np.sum(p_eat_outcomes*u_eat_outcomes)\n","\n","print(\"eu eat: \",eu_eat)\n","print(\"-----------------\")\n","\n","print('choice meu: ',np.max(np.array([eu_coffee,eu_clothes,eu_eat])))\n","print('choice maxdc: ',np.max(np.array([np.max(u_clothes_outcomes),np.max(u_coffee_outcomes),np.max(u_eat_outcomes)])))\n","print('choice mindc: ',np.max(np.array([np.min(u_clothes_outcomes),np.min(u_coffee_outcomes),np.min(u_eat_outcomes)])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJxxWoUs9Qg3","executionInfo":{"status":"ok","timestamp":1683797885923,"user_tz":-120,"elapsed":8,"user":{"displayName":"Michele Sprocatti","userId":"03401605544328529212"}},"outputId":"0bf0e992-e85c-4cd2-929c-22499860ad0b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["eu coffee:  7.0\n","eu clothes:  -7.0\n","eu eat:  4.4\n","-----------------\n","choice meu:  7.0\n","choice maxdc:  20\n","choice mindc:  2\n"]}]},{"cell_type":"markdown","source":["## EXERCISE 2: Solving a MDP with MDP toolbox\n","\n","We have four states and four actions.\n","\n","The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n","\n","The states are 0, 1, 2, 3, and they are arranged like this:\n","    \n","$$\n","\\begin{array}{cc}\n","2 & 3\\\\\n","0 & 1\\\\\n","\\end{array}\n","$$\n","\n","The motion model provides:\n","*   0.8 probability of moving in the direction of the action,\n","*   0.1 probability of moving in each of the directions perpendicular to that of the action.\n","\n","So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n","\n","In case of \"infeasible\" movements, the agent remains in the current state.\n","\n","The reward for state 3 is 1, and the reward for state 1 is -1, and the agent does not leave those states.\n","\n","Set discount factor equal to 0.99.\n","\n","> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n","\n","> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n","\n","> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n","\n","> __QUESTION 2(d):__ Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions.\n"],"metadata":{"id":"wRLlnsWJ90DZ"}},{"cell_type":"code","source":["!pip install pymdptoolbox\n","import mdptoolbox"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ft4nETgmA0za","executionInfo":{"status":"ok","timestamp":1683797900266,"user_tz":-120,"elapsed":14348,"user":{"displayName":"Michele Sprocatti","userId":"03401605544328529212"}},"outputId":"28aec62d-3803-4372-e421-f9aa8761d11b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pymdptoolbox\n","  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.10.1)\n","Building wheels for collected packages: pymdptoolbox\n","  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25656 sha256=46d32458902d5927e2e8f798c84d62c4747bc46bdef90a21153a0540a45abc34\n","  Stored in directory: /root/.cache/pip/wheels/2b/e7/c7/d7abf9e309f3573a934fed2750c70bd75d9e9d901f7f16e183\n","Successfully built pymdptoolbox\n","Installing collected packages: pymdptoolbox\n","Successfully installed pymdptoolbox-4.0b3\n"]}]},{"cell_type":"code","source":["# 0 1 2 3\n","P1=np.array([\n","            [[0.1,0.8,0.1,0], #azione 0 stato 0\n","            [0,1,0,0],#azione 0 stato 1\n","            [0.1,0,0.1,0.8], #azione 0 stato 2\n","            [0,0,0,1]],#azione 0 stato 3\n","             \n","            [[0.9,0,0.1,0],#azione 1 stato 0\n","             [0,1,0,0],#azione 1 stato 1\n","             [0.1,0,0.9,0],#azione 1 stato 2\n","             [0,0,0,1]],#azione 1 stato 3\n","             \n","            [[0.1,0.1,0.8,0],#azione 2 stato 0\n","             [0,1,0,0],#azione 2 stato 1\n","             [0,0,0.9,0.1],#azione 2 stato 2\n","             [0,0,0,1]],#azione 2 stato 3\n","             \n","            [[0.9,0.1,0,0],#azione 3 stato 0\n","             [0,1,0,0],#azione 3 stato 1\n","             [0.8,0,0.1,0.1],#azione 3 stato 2\n","             [0,0,0,1]],#azione 3 stato 3\n","\n","             ])\n","\n","R1=np.array([[-1, -0.04,-0.04,-0.04], [-1, -1, -1, -1], [1,-0.04,-0.04, -0.04], [1, 1, 1, 1]])\n","\n","mdptoolbox.util.check(P1, R1)\n","\n","v1=mdptoolbox.mdp.ValueIteration(P1,R1,0.99)\n","#v1.setVerbose()\n","print(\"--------------------------------\")\n","v1.run()\n","print(\"policiy value iteration: \",v1.policy)\n","\n","v1=mdptoolbox.mdp.PolicyIteration(P1,R1,0.99)\n","#v1.setVerbose()\n","print(\"--------------------------------\")\n","v1.run()\n","print(\"policy policy iteration: \",v1.policy)\n","\n","v1=mdptoolbox.mdp.QLearning(P1,R1,0.99)\n","#v1.setVerbose()\n","print(\"--------------------------------\")\n","v1.run()\n","print(\"policy q-learning: \",v1.policy)"],"metadata":{"id":"gnaXtVwxBCdJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683797900651,"user_tz":-120,"elapsed":394,"user":{"displayName":"Michele Sprocatti","userId":"03401605544328529212"}},"outputId":"b80fb991-31ac-4d9c-e471-4f06b4c90362"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------\n","policiy value iteration:  (1, 0, 0, 0)\n","--------------------------------\n","policy policy iteration:  (1, 0, 0, 0)\n","--------------------------------\n","policy q-learning:  (2, 1, 0, 0)\n"]}]}]}